{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7085f899-d42d-4995-a126-302df46f975b",
   "metadata": {},
   "source": [
    "# Traduire le langage en probabilités"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62fa8b6-e005-4c70-9b20-1f6aa16a5409",
   "metadata": {},
   "source": [
    "Quelle est la probabilité de tirer la lettre *A* au *Scrabble* ? La question, anodine, ne peut se résoudre que si nous connaissons deux quantités :\n",
    "\n",
    "- le nombre total de jetons ;\n",
    "- le nombre de jetons *A*.\n",
    "\n",
    "Quelle est maintenant la probabilité de réalisation de l’événement *i* dans le verbe *boire* ? La réponse est le quotient entre le nombre de lettres *i* dans le verbe donné avec le nombre total de lettres qui le composent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf48199-709a-4bf8-a831-5dd69cb070a5",
   "metadata": {},
   "source": [
    "## La fréquence relative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6787e4be-9d57-4e31-abec-f9db3508c97a",
   "metadata": {},
   "source": [
    "Dénombrer les lettres d’un texte ne donne qu’une mesure absolue de la présence de chacune, sans ne rien dire de leur importance. Qu’une lettre apparaisse trois mille fois est en soi beaucoup, mais au milieu d’un corpus de trois milliards de mots, elle ne pèse guère, d’où la nécessité de toujours considérer un chiffre dans son contexte.\n",
    "\n",
    "Plus formellement, la relation précédente s’exprime par l’équation :\n",
    "\n",
    "$$\n",
    "P(c) = \\frac{F(c)}{N}\n",
    "$$\n",
    "\n",
    "Dans la formule, $P(c)$ est la probabilité de réalisation de l’événement $c$ – à savoir un caractère, $F(c)$ la fréquence d’apparition du caractère et $N$ la taille du corpus. En l’appliquant au verbe plus haut, on peut en déduire que la probabilité de l’événement *i* est égale à :\n",
    "\n",
    "$$\n",
    "P(i) = \\frac{1}{5} = 0,2\n",
    "$$\n",
    "\n",
    "Dans ce contexte, la réalisation d’un événement aléatoire est jugée indépendante des autres événements. Ainsi, la probabilité de réaliser l’événement *i* puis l’événement *r*, soit d’obtenir exactement la séquence *ir* revient à multiplier les probabilités des deux événements :\n",
    "\n",
    "$$\n",
    "P(i \\cap r) = P(i) \\times P(r) = 0,2 \\times 0,2 = 0,04\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586ccea5-39f0-4e9a-8fdd-c45b1109bc0b",
   "metadata": {},
   "source": [
    "## Un modèle *n*-grammes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a255ebcc-9510-459f-bd74-5df3afb324ad",
   "metadata": {},
   "source": [
    "Considérons la phrase suivante :\n",
    "\n",
    ">Le petit chat boit du lait.\n",
    "\n",
    "Quelle serait maintenant la probabilité que l’événement *it* se produise ? La question est différente de la précédente comme on considère l’ensemble *it* comme un événement et plus comme deux événements distincts. Il est par conséquent possible d’appliquer la première formule :\n",
    "\n",
    "$$\n",
    "P(it) = \\frac{F(it)}{N-1}\n",
    "$$\n",
    "\n",
    "Pourquoi $N-1$ ? Les fréquences calculées précédemment n’impliquaient que des unigrammes (ou 1-grammes) or, ici, nous souhaitons estimer la probabilité d’un bigramme (ou 2-grammes). La règle est linéaire, de telle manière qu’il existe $N-2$ trigrammes, $N-3$ tétragrammes, $N-4$ 5-grammes etc. Considérant qu’il y a 27 caractères dans la phrase, espaces et ponctuation comprises, la probabilité de l’événement *it* s’établit ainsi à :\n",
    "\n",
    "$$\n",
    "P(it) = \\frac{3}{27 - 1} \\approx 0.1154\n",
    "$$\n",
    "\n",
    "Dans un modèle *n*-grammes toutefois, la question n’est pas réellement d’estimer la probabilité d’un événement mais plutôt sa vraisemblance en fonction d’un historique. Reformulons : quelle serait la probabilité de l’événement *t* sachant que *i* est arrivé juste avant ? Du point de vue mathématique, la formule revient à :\n",
    "\n",
    "$$\n",
    "P(t|i) = \\frac{F(it)}{F(i)} = \\frac{3}{3} = 1\n",
    "$$\n",
    "\n",
    "En effet, dans le corpus, un *i* est toujours suivi d’un *t*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55192e12-4558-413f-993d-c65c1b319b97",
   "metadata": {},
   "source": [
    "## Des contraintes pesant sur les modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5737c0e0-5d08-49e9-9abc-c6a7d228f489",
   "metadata": {},
   "source": [
    "### Formule des probabilités composées"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4480ff04-1f2c-4a3a-8904-aee54d75af65",
   "metadata": {},
   "source": [
    "L’intuition première pour calculer des probabilités sur des chaînes de caractères serait d’appliquer une chaîne de traitement sans historicité :\n",
    "\n",
    "$$\n",
    "P(u_1 u_2 \\ldots u_n) \\approx \\prod_i P(u_i)\n",
    "$$\n",
    "\n",
    "Pour obtenir le mot *chai*, il suffirait ainsi de multiplier entre elles les probabilités de chacune des lettres :\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(chai) &= P(c) \\cdot P(h) \\cdot P(a) \\cdot P(i)\\\\\n",
    "&= \\frac{1}{27} \\cdot \\frac{1}{27} \\cdot \\frac{2}{27} \\cdot \\frac{3}{27} \\approx 1,129 \\times 10^{-5}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Et dans un modèle bigramme, la logique reste la même, chaque unité étant cette fois-ci composée de deux caractères :\n",
    "\n",
    "$$\n",
    "P(chai) = P(ch) \\cdot P(ha) \\cdot P(ai) = \\frac{1}{26} \\cdot \\frac{1}{26} \\cdot \\frac{1}{26} = 0,0000569\n",
    "$$\n",
    "\n",
    "Ce genre de modèle qui ne tient pas compte du contexte reste très limité : un générateur basé sur une telle règle écrirait des caractères aléatoirement. Après tout, les chaînes *du* et *hb* bénéficient de la même estimation de probabilité alors que le bigramme *du* existe dans le corpus mais pas *hb* :\n",
    "\n",
    "$$\n",
    "P(du) = P(d) * P(u) \\approx 0.0384 \\cdot 0.0384 \\approx 0.001479\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(hb) = P(h) * P(b) \\approx 0.0384 \\cdot 0.0384 \\approx 0.001479\n",
    "$$\n",
    "\n",
    "Une meilleure approche consiste à poser le problème sous forme de probabilités conditionnelles :\n",
    "\n",
    "$$\n",
    "P(chai) = P(h|c) \\cdot P(a|ch) \\cdot P(i|cha)\n",
    "$$\n",
    "\n",
    "Une formule qui peut se généraliser :\n",
    "\n",
    "$$\n",
    "P(u_1 u_2 \\ldots u_n) = \\prod_{i=1} P(u_i ∣ u_0 \\ldots u_{i−1})\n",
    "$$\n",
    "\n",
    "Si elle semble prometteuse, sur l’argument qu’elle repose toujours sur l’antériorité, elle aboutit très rapidement à une probabilité de 0 pour des énoncés pourtant probables :\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(chai) &= P(h|c) \\cdot P(a|ch) \\cdot P(i|cha)\\\\\n",
    "&= \\frac{F(ch)}{F(c)} \\cdot \\frac{F(cha)}{F(ch)} \\cdot \\frac{F(chai)}{F(cha)}\\\\\n",
    "&= \\frac{1}{1} \\cdot \\frac{1}{1} \\cdot \\frac{0}{1}\\\\\n",
    "&= 1 \\cdot 1 \\cdot 0\\\\\n",
    "&= 0\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d971ef84-19d2-48d4-adea-108fbd167795",
   "metadata": {},
   "source": [
    "### Hypothèses de correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83de50a9-627d-403b-91d1-144a26705123",
   "metadata": {},
   "source": [
    "Pour éviter cet écueil, la première intuition serait d’ignorer tout simplement les cas qui n’apparaissent pas dans le corpus d’entraînement. Si nous retenons cette idée, $P(chai)$ devient subitement égale à 1. Passer de 0 à 100 % de chances de voir le mot *chai* (autant que le mot *chat*) semble indiquer que notre modèle de langage n’est pas très rationnel.\n",
    "\n",
    "Une correction plus envisageable, et qui prend le nom de **lissage de Laplace**, serait d’augmenter de 1 toutes les fréquences du corpus et d’assigner la même valeur aux évènements nuls :\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(chat) &= \\frac{F(ch)+1}{F(c)+1} \\cdot \\frac{F(cha)+1}{F(ch)+1} \\cdot \\frac{F(chat)+1}{F(cha)+1}\\\\\n",
    "&= \\frac{2}{2} \\cdot \\frac{2}{2} \\cdot \\frac{2}{2}\\\\\n",
    "&= 1\\\\\n",
    "P(chai) &= \\frac{F(ch)+1}{F(c)+1} \\cdot \\frac{F(cha)+1}{F(ch)+1} \\cdot \\frac{F(chai)+1}{F(cha)+1}\\\\\n",
    "&= \\frac{2}{2} \\cdot \\frac{2}{2} \\cdot \\frac{1}{2}\\\\\n",
    "&= 0.5\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "En poursuivant cette logique, une meilleure approximation de la probabilité d’un évènement inconnu s’appuierait sur une pondération du maximum de vraisemblance. Fixons le paramètre $\\alpha$ à 0,05 pour la probabilité d’un évènement inconnu et un paramètre $k$ pour la pondération qui vaut $1 - \\alpha$ soit 0,95 :\n",
    "\n",
    "$$\n",
    "P(chai) = 0.95 \\times 0 + \\frac{0.05}{27} \\approx 0.0019\n",
    "$$\n",
    "\n",
    "La formule vaut ainsi :\n",
    "\n",
    "$$\n",
    "P(w_i) = k \\cdot P_{ML}(w_i) + \\frac{\\alpha}{N}\n",
    "$$\n",
    "\n",
    "Dans la réalité, le paramètre $k$ est ajusté sur le corpus d’entraînement plutôt que déterminé arbitrairement. D’autres techniques existent, comme les actualisations de Good-Turing et de Witten-Bell, ou encore le lissage de Kneser-Ney qui estiment les évènements inconnus sur la base des hapax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53548611-0d5c-4e66-b6b2-0f7caa84178b",
   "metadata": {},
   "source": [
    "### L’hypothèse markovienne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481769ba-dc65-4fd1-9864-3c34f3d97e33",
   "metadata": {},
   "source": [
    "En théorie, la formule des probabilités composées couvre bien les besoins impliqués par la construction d’un modèle de langage simple. Après tout, quand un corpus d’apprentissage n’est constitué que de quelques caractères et que le nombre de caractères d’un mot est fini, il semble envisageable de calculer toutes les possibilités.\n",
    "\n",
    "Étendons notre exemple au français, qui dispose d’un alphabet de 26 lettres et dont le mot le plus long, *anticonstitutionnellement*, en compte 25. Là encore, générer un mot de *n* caractères en partant d’un état initial et en se fondant sur des données d’entraînement suffisamment importantes semble à portée de main. Oui, mais… qu’en est-il des mots composés ? Et dans les autres langues ? Les règles de composition de l’allemand font par exemple le bonheur des écoliers qui inventent les péripéties d’un capitaine naviguant sur le Danube et le Rhin (*Donaurheinschifffahrtskapitän*) afin d’égaler la prouesse établie par la *Donau­dampf­schiffahrts­elektrizitäten­haupt­betriebs­werk­bau­unter­beamten­gesellschaft*.\n",
    "\n",
    "Rappelons aussi que l’unité de base des modèles de langage n’est pas le caractère mais le mot. L’alphabet devient lexique et la longueur d’une phrase étant virtuellement infinie, les besoins en calcul sont si délirants qu’il ne sera jamais possible d’estimer de telles probabilités.\n",
    "\n",
    "Comment faire alors ? C’est ici qu’intervient l’hypothèse markovienne, appelée ainsi après le mathématicien russe Andreï Markov, qui suggère que la probabilité conditionnelle d’un mot sachant son historique est approximée par la probabilité de ce mot sachant uniquement celui qui le précède. Par exemple, la probabilité que le mot *flots* termine la phrase *Le soleil brille au-dessus des* est proche de celle que le mot *flots* suive directement l’article *des* :\n",
    "\n",
    "$$\n",
    "P(\\text{flots}|\\text{Le soleil brille au-dessus des}) \\approx P(\\text{flots}|\\text{des})\n",
    "$$\n",
    "\n",
    "On parle aussi d’**horizon limité**, une approximation qui peut s’écrire sous la forme :\n",
    "\n",
    "$$\n",
    "P(w_{n+1}|w_1 \\ldots w_n) \\approx P(w_{n+1}|w_n)\n",
    "$$\n",
    "\n",
    "Mieux, l’hypothèse markovienne dans sa généralisation définit une fenêtre contextuelle à gauche :\n",
    "\n",
    "$$\n",
    "P(w_1 w_2 \\ldots w_n) \\approx \\prod_i P(w_i|w_{i-k} \\ldots w_{i-1})\n",
    "$$\n",
    "\n",
    "Les modèles probabilistes peuvent s’étendre ainsi aux trigrammes, aux tétragrammes, aux… sauf que le langage fait montre de tant de diversité que les modèles *n*-grammes ne suffisent généralement pas. Prenons un énoncé :\n",
    "\n",
    "> Le film, encensé par la critique de nombreux médias spécialisés, comme *Les inrockuptibles*, *Télérama*, *ÉcranLarge* ou encore *Première*, qui s’était déjà exprimée favorablement sur l’opus précédent, est fort mauvais.\n",
    "\n",
    "La structure des phrases, notamment à l’écrit, implique souvent une dépendance longue distance (*long distance dependancy* ou LDD) facilitée par des incises, des relatives ou des tournures interrogatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4135ac6-2453-4d5b-95a8-ee106ef2447c",
   "metadata": {},
   "source": [
    "## Étiquetage et décision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e88b4d2-7ac6-4a38-9e1c-d9eca4fae66f",
   "metadata": {},
   "source": [
    "Reprenons un exemple bien connu d’étiquetage d’un énoncé ambigu :\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A(\"La\n",
    "    DET\"):::academic-->B(\"petite\n",
    "    N\"):::formal\n",
    "    B-->C(\"brise\n",
    "    V\"):::formal\n",
    "    C-->D(\"la\n",
    "    DET\"):::formal\n",
    "    D-->E(\"glace\n",
    "    N\"):::formal\n",
    "    E-->F(\".\n",
    "    PONCT\"):::academic\n",
    "    A-->G(\"petite\n",
    "    ADJ\"):::academic\n",
    "    G-->H(\"brise\n",
    "    N\"):::academic\n",
    "    H-->I(\"la\n",
    "    PRO\"):::academic\n",
    "    I-->J(\"glace\n",
    "    V\"):::academic\n",
    "    J-->F\n",
    "    classDef formal fill:#fff,stroke:#D68738,color:#D68738\n",
    "    classDef academic fill:#032B4F,stroke:#032B4F,color:#fff\n",
    "```\n",
    "\n",
    "Ces deux voies ne sont pas équiprobables et l’interprétation du mot *petite* dépendra fortement du corpus d’apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268b690c-fd26-4980-ab07-2f35369d3e9b",
   "metadata": {},
   "source": [
    "### Analyse d’un unigramme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf94bc1-e23f-45b8-ae4b-985d8e6b35e3",
   "metadata": {},
   "source": [
    "Pour un mot *w* donné, on peut déterminer son étiquette *t* grâce à sa fréquence d’apparition dans le corpus :\n",
    "\n",
    "$$\n",
    "P(t|w) = \\frac{F(w,t)}{F(w)}\n",
    "$$\n",
    "\n",
    "Si le mot *petite* apparaît 38 fois dont 12 en tant que nom, la probabilité de le voir avec l’étiquette *ADJ* est de :\n",
    "\n",
    "$$\n",
    "P(ADJ|petite) = \\frac{26}{38} = 0,6842\n",
    "$$\n",
    "\n",
    "À noter que la probabilité de *t* sachant *w* peut s’écrire comme un rapport entre la probabilité du couple mot/étiquette et la probabilité du mot :\n",
    "\n",
    "$$\n",
    "P(t|w) = \\frac{P(t,w)}{P(w)}\n",
    "$$\n",
    "\n",
    "Dans notre exemple fictif, imaginons que nous avons entraîné notre étiqueteur sur un corpus de 1000 mots. La formule donnerait :\n",
    "\n",
    "$$\n",
    "P(ADJ|petite) = \\frac{\\frac{26}{1000}}{\\frac{38}{1000}} = 0,6842\n",
    "$$\n",
    "\n",
    "De manière réciproque, il serait possible de déduire le mot sachant l’étiquette. Toutefois, la précision dans ce sens sera bien moindre : quand on hésite seulement entre quelques étiquettes pour un mot, la quantité de mots possibles pour une étiquette est bien plus importante.\n",
    "\n",
    "Ce modèle est jugé simpliste : non seulement il choisit parmi plusieurs possibilités la plus probable ($P(ADJ|petite) > P(N|petite)$) mais en plus il ne tient pas compte de l’antériorité. Au mot suivant, *brise*, il pourrait choisir d’attribuer l’étiquette *V*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e52ab9-b96c-465b-8f38-29343b9b589a",
   "metadata": {},
   "source": [
    "### Étiquetage *n*-gramme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e1f8c0-f807-4a89-b4cb-0d915dd2f7d2",
   "metadata": {},
   "source": [
    "#### Définition de la tâche"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9a7af9-9a9e-49f8-bcde-85493ea14518",
   "metadata": {},
   "source": [
    "L’idée sous-jacente des étiqueteurs *n*-gramme est que l’étiquette d’un mot dépend des *n-1* mots qui le précèdent. Dans la pratique, comme les ressources nécessaires pour le calcul des probabilités croissent avec *n* et que la fiabilité des résultats diminue, notamment en raison d’un phénomène de dilution, on se contente d’une faible valeur de *n*.\n",
    "\n",
    "Trois hypothèses sont retenues :\n",
    "\n",
    "- l’approche bayésienne  \n",
    "$P(t|w) = \\frac{P(w|t) \\cdot P(t)}{P(w)}$\n",
    "- l’horizon limité de Markov  \n",
    "$P(t_n|t_{1,n-1}) = P(t_n|t_{n-1})$\n",
    "- la probablité d’apparition d’un mot pour une étiquette ne dépend que de son étiquette  \n",
    "$P(w_i|t_{1,n}) = P(w_i|t_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd71ec19-62d4-4e8f-b0ea-b2a2ef2d7a2d",
   "metadata": {},
   "source": [
    "#### Paramètres de résolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af96d07-1337-4e93-89da-e9d3826572d0",
   "metadata": {},
   "source": [
    "Soient deux ensembles *W* et *T* pour les mots et les étiquettes. À une séquence de *n* mots est associée une séquence de *n* étiquettes de telle manière que :\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  w_{1,n} &= w_1 w_2 \\ldots w_n\\\\\n",
    "  t_{1,n} &= t_1 t_2 \\ldots t_n\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "L’objectif, sachant $w_{1,n}$, est de déterminer $t_{1,n}$ en maximisant $P(t_{1,n}|w_{1,n})$, c’est-à-dire, après avoir calculé toutes les probabilités possibles, de ne conserver que la plus forte :\n",
    "\n",
    "$$\n",
    "t_{1,n} = max_{t_{1,n}} P(t_{1,n}|w_{1,n})\n",
    "$$\n",
    "\n",
    "Ce qui, d’après le théorème de Bayes, s’exprime en renversant la condition :\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  P(t_{1,n}|w_{1,n}) &= \\frac{P(w_{1,n}|t_{1,n}) \\cdot P(t_{1,n})}{P(w_{1,n})}\\\\\n",
    "  t_{1,n} &= \\text{max} P(w_{1,n}|t_{1,n}) \\cdot P(t_{1,n})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Il reste deux probabilités à calculer pour lesquelles on applique les hypothèses retenues plus haut :\n",
    "\n",
    "- $P(w_{1,n}|t_{1,n})$ : hypothèse selon laquelle un mot ne dépend que de son étiquette\n",
    "- $P(t_{1,n})$ : hypothèse selon laquelle une étiquette dépend de ses *n* étiquettes précédentes\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  P(w_{1,n}|t_{1,n}) &= P(w_1|t_1) \\cdot P(w_2|t_2) \\ldots P(w_n|t_n)\\\\\n",
    "  P(t_{1,n}) &= P(t_1) \\cdot P(t_2|t_1) \\cdot P(t_3|t_2) \\ldots P(t_n|t_{n−1})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a3566f-8c2e-4559-9de6-fd10c70f77c5",
   "metadata": {},
   "source": [
    "### Modèle de Markov caché d’ordre *N*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5fb235-0f52-48a9-8513-4188001e07b0",
   "metadata": {},
   "source": [
    "Le modèle décrit plus haut est appelé modèle de Markov caché d’ordre 1 (*Hidden Markov Model*, ou HMM) dans la mesure où la probabilité d’apparition d’une étiquette ne dépend que de la précédente. Pour un ordre plus élevé, par exemple pour un HMM d’ordre 2, nous aurons :\n",
    "\n",
    "$$\n",
    "P(t_{1,n}) = P(t_1) \\cdot P(t_2|t_1) \\cdot P(t_3|t_1 t_2) \\ldots P(t_n|t_{n−2}t_{n−1})\n",
    "$$\n",
    "\n",
    "En appliquant un HMM, la tâche pour un étiquetage *n*-gramme se formulerait ainsi :\n",
    "\n",
    "$$\n",
    "t_{1,n} = \\text{max} P(w_1|t_1) \\cdot \\prod_{i=1,n} P(t_i|t_{i-1}) \\cdot P(w_i|t_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b8fae8-3f00-4767-b9bc-64fddb01aabf",
   "metadata": {},
   "source": [
    "## Étude de cas : entraînement d’un HMM d’ordre 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792dbac4-91a1-4504-acb5-4bfb14583a63",
   "metadata": {},
   "source": [
    "Nous souhaitons calculer la probabilité de génération des trois phrases suivantes :\n",
    "\n",
    "> (A) Le chat mange la souris.  \n",
    "> (B) La souris mange le chat.  \n",
    "> (C) La souris mange.\n",
    "\n",
    "Pour résoudre la tâche, nous souhaitons utiliser un modèle de Markov à états cachés d’ordre 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a353b84e-d7e1-4dea-82b4-a927e995a8b7",
   "metadata": {},
   "source": [
    "### Définition du corpus d’apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa979dc2-01a5-4b54-920c-0e612aee89e5",
   "metadata": {},
   "source": [
    "Considérons un corpus étiqueté de cinq phrases où les étiquettes en parties du discours constituent les états cachés du modèle de Markov :\n",
    "\n",
    "> (1) Le/DET chat/NOM mange/VER une/DET souris/NOM ./PONCT  \n",
    "> (2) La/DET fille/NOM lit/VER un/DET livre/NOM ./PONCT  \n",
    "> (3) Un/DET chien/NOM court/VER dans/PREP le/DET jardin/NOM ./PONCT  \n",
    "> (4) Nous/PRON mangeons/VER des/DET fruits/NOM frais/ADJ ./PONCT  \n",
    "> (5) Le/DET petit/ADJ garçon/NOM joue/VER au/PREP+DET ballon/NOM ./PONCT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a9a69f-7351-433b-afac-fc0a5caebbd7",
   "metadata": {},
   "source": [
    "### Étape 1 : calculer les probabilités d’émission des étiquettes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e491614b-a15d-4eed-ad1c-ecdba0797603",
   "metadata": {},
   "source": [
    "La première étape consiste à calculer les probabilités d’émission pour chaque mot sachant son étiquette. Par exemple, il y a huit déterminants dans le corpus, dont trois *le*, soit :\n",
    "\n",
    "$$\n",
    "P(\\text{le}|\\text{DET}) = \\frac{3}{8} = 0,375\n",
    "$$\n",
    "\n",
    "Du point de vue mathématique, on peut généraliser la relation avec la formule :\n",
    "\n",
    "$$\n",
    "P(w_i|t_i) = \\frac{F(w_i, t_i)}{F(t_i)}\n",
    "$$\n",
    "\n",
    "La tableau ci-dessous reprend toutes les probabilités d’émission :\n",
    "\n",
    "|$w, t$   | DET  | NOM  | VER  | PONCT | PREP | PRON | ADJ  | PREP+DET |\n",
    "|---------|------|------|------|-------|------|------|------|---------|\n",
    "| **le**  | 0.375 | 0    | 0    | 0     | 0    | 0    | 0    | 0       |\n",
    "| **une** | 0.125 | 0    | 0    | 0     | 0    | 0    | 0    | 0       |\n",
    "| **la**  | 0.125 | 0    | 0    | 0     | 0    | 0    | 0    | 0       |\n",
    "| **un**  | 0.250 | 0    | 0    | 0     | 0    | 0    | 0    | 0       |\n",
    "| **des** | 0.125 | 0    | 0    | 0     | 0    | 0    | 0    | 0       |\n",
    "| **chat** | 0    | 0.111 | 0    | 0     | 0    | 0    | 0    | 0       |\n",
    "| **souris** | 0    | 0.111 | 0    | 0     | 0    | 0    | 0    | 0       |\n",
    "| **fille** | 0    | 0.111 | 0    | 0     | 0    | 0    | 0    | 0       |\n",
    "| **livre** | 0    | 0.111 | 0    | 0     | 0    | 0    | 0    | 0       |\n",
    "| **chien** | 0    | 0.111 | 0    | 0     | 0    | 0    | 0    | 0       |\n",
    "| **jardin** | 0    | 0.111 | 0    | 0     | 0    | 0    | 0    | 0       |\n",
    "| **fruits** | 0    | 0.111 | 0    | 0     | 0    | 0    | 0    | 0       |\n",
    "| **garçon** | 0    | 0.111 | 0    | 0     | 0    | 0    | 0    | 0       |\n",
    "| **ballon** | 0    | 0.111 | 0    | 0     | 0    | 0    | 0    | 0       |\n",
    "| **mange** | 0    | 0    | 0.200 | 0     | 0    | 0    | 0    | 0       |\n",
    "| **lit** | 0    | 0    | 0.200 | 0     | 0    | 0    | 0    | 0       |\n",
    "| **court** | 0    | 0    | 0.200 | 0     | 0    | 0    | 0    | 0       |\n",
    "| **mangeons** | 0    | 0    | 0.200 | 0     | 0    | 0    | 0    | 0       |\n",
    "| **joue** | 0    | 0    | 0.200 | 0     | 0    | 0    | 0    | 0       |\n",
    "| **.** | 0    | 0    | 0    | 1.000 | 0    | 0    | 0    | 0       |\n",
    "| **dans** | 0    | 0    | 0    | 0     | 1.000 | 0    | 0    | 0       |\n",
    "| **nous** | 0    | 0    | 0    | 0     | 0    | 1.000 | 0    | 0       |\n",
    "| **frais** | 0    | 0    | 0    | 0     | 0    | 0    | 0.500 | 0       |\n",
    "| **petit** | 0    | 0    | 0    | 0     | 0    | 0    | 0.500 | 0       |\n",
    "| **au** | 0    | 0    | 0    | 0     | 0    | 0    | 0    | 1.000   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0257bb-4c41-4378-94ae-5bd9d8f271cc",
   "metadata": {},
   "source": [
    "### Étape 2 : calculer les probabilités de transition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bcfb3b-6319-49bd-8bea-f279ea075905",
   "metadata": {},
   "source": [
    "Un HMM d’ordre 1 n’estime la probabilité d’apparition d’une étiquette qu’en fonction de celle qui la précède directement :\n",
    "\n",
    "$$\n",
    "P(t_i|t_{i-1}) = \\frac{F(t_{i-1} t_i)}{F(t_{i-1})}\n",
    "$$\n",
    "\n",
    "Cette relation nous permet de calculer les probabilités de transition d’un état à l’autre. Par exemple, sur huit occurrences, un déterminant est suivi sept fois par un nom contre une seule fois par un adjectif. Soit :\n",
    "\n",
    "$$\n",
    "P(\\text{NOM}|\\text{DET}) = \\frac{7}{8} = 0,875\n",
    "$$\n",
    "\n",
    "Nous reportons ci-dessous toutes les probabilités de transition :\n",
    "\n",
    "|$t_i, t_{i+1}$  | START | DET  | NOM  | VER  | PONCT | PREP | PRON | ADJ  | PREP+DET |\n",
    "|---------------|-------|------|------|------|-------|------|------|------|---------|\n",
    "| **START**    | 0     | 0.800 | 0    | 0    | 0     | 0    | 0.200 | 0    | 0       |\n",
    "| **DET**      | 0     | 0     | 0.875 | 0    | 0     | 0    | 0     | 0.125 | 0       |\n",
    "| **NOM**      | 0     | 0     | 0    | 0.444 | 0.444 | 0    | 0     | 0.111 | 0       |\n",
    "| **VER**      | 0     | 0.600 | 0    | 0    | 0     | 0.200 | 0     | 0    | 0.200   |\n",
    "| **PONCT**    | 0     | 0     | 0    | 0    | 0     | 0    | 0     | 0    | 0       |\n",
    "| **PREP**     | 0     | 1.000 | 0    | 0    | 0     | 0    | 0     | 0    | 0       |\n",
    "| **PRON**     | 0     | 0     | 0    | 1.000 | 0     | 0    | 0     | 0    | 0       |\n",
    "| **ADJ**      | 0     | 0     | 0.500 | 0    | 0.500 | 0    | 0     | 0    | 0       |\n",
    "| **PREP+DET** | 0     | 0     | 1.000 | 0    | 0     | 0    | 0     | 0    | 0       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f7ba15-cc03-423f-a2ad-89cd62b9d6b9",
   "metadata": {},
   "source": [
    "### Étape 3 : établir le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec6100f-9f99-4329-9162-e275ee71bafe",
   "metadata": {},
   "source": [
    "Si nous reprenons la définition d’un modèle de Markov caché d’ordre *N*, nous pouvons formaliser l’expression suivante :\n",
    "\n",
    "$$\n",
    "P(w_1, \\dots, w_n, t_1, \\dots, t_n) = P(t_1 | \\text{START}) \\cdot P(w_1 | t_1) \\cdot \\prod_{i=2}^{n} P(t_i | t_{i-1}) \\cdot P(w_i | t_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464de0c5-7a58-4026-91ba-e53bf6f2b7c9",
   "metadata": {},
   "source": [
    "### Étape 4 : extraire les probabilités d’émission et de transition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ec21ff-e367-4c16-9866-40991018524b",
   "metadata": {},
   "source": [
    "#### Probabilités d’émission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29fd170-80e6-473d-b107-64b0905de4b1",
   "metadata": {},
   "source": [
    "D’après les matrices calculées aux étapes précédentes, nous pouvons extraire les probabilités d’émission des mots de nos phrases en fonction de leurs étiquettes :\n",
    "\n",
    "- A :\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  P(\\text{le} ∣ \\text{DET}) &= 0,375\\\\\n",
    "  P(\\text{chat} ∣ \\text{NOM}) &= 0,111\\\\\n",
    "  P(\\text{mange} ∣ \\text{VER}) &= 0,200\\\\\n",
    "  P(\\text{la} ∣ \\text{DET}) &= 0,125\\\\\n",
    "  P(\\text{souris} ∣ \\text{NOM}) &= 0,111\\\\\n",
    "  P(\\text{.} ∣ \\text{PONCT}) &= 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- B :\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  P(\\text{la} ∣ \\text{DET}) &= 0,125\\\\\n",
    "  P(\\text{souris} ∣ \\text{NOM}) &= 0,111\\\\\n",
    "  P(\\text{mange} ∣ \\text{VER}) &= 0,200\\\\\n",
    "  P(\\text{le} ∣ \\text{DET}) &= 0,375\\\\\n",
    "  P(\\text{chat} ∣ \\text{NOM}) &= 0,111\\\\\n",
    "  P(\\text{.} ∣ \\text{PONCT}) &= 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- C :\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  P(\\text{la} ∣ \\text{DET}) &= 0,125\\\\\n",
    "  P(\\text{souris} ∣ \\text{NOM}) &= 0,111\\\\\n",
    "  P(\\text{mange} ∣ \\text{VER}) &= 0,200\\\\\n",
    "  P(\\text{.} ∣ \\text{PONCT}) &= 1\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f186f4ff-5e0f-4b02-947a-fa74f5676d98",
   "metadata": {},
   "source": [
    "#### Probabilités de transition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16e1975-9950-4d21-9ca9-99fb312bff77",
   "metadata": {},
   "source": [
    "De façon similaire, nous pouvons reporter les probabilités de transition :\n",
    "\n",
    "- A :\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  P(\\text{DET}∣\\text{START}) &= 0,8\\\\\n",
    "  P(\\text{NOM}∣\\text{DET}) &= 0,875\\\\\n",
    "  P(\\text{VER}∣\\text{NOM}) &= 0,444\\\\\n",
    "  P(\\text{DET}∣\\text{VER}) &= 0,6\\\\\n",
    "  P(\\text{NOM}∣\\text{DET}) &= 0,875\\\\\n",
    "  P(\\text{PONCT}∣\\text{NOM}) &= 0,444\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- B :\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  P(\\text{DET}∣\\text{START}) &= 0,8\\\\\n",
    "  P(\\text{NOM}∣\\text{DET}) &= 0,875\\\\\n",
    "  P(\\text{VER}∣\\text{NOM}) &= 0,444\\\\\n",
    "  P(\\text{DET}∣\\text{VER}) &= 0,6\\\\\n",
    "  P(\\text{NOM}∣\\text{DET}) &= 0,875\\\\\n",
    "  P(\\text{PONCT}∣\\text{NOM}) &= 0,444\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- C :\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  P(\\text{DET}∣\\text{START}) &= 0,8\\\\\n",
    "  P(\\text{NOM}∣\\text{DET}) &= 0,875\\\\\n",
    "  P(\\text{VER}∣\\text{NOM}) &= 0,444\\\\\n",
    "  P(\\text{PONCT}∣\\text{VER}) &= 0\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5976dbae-4df3-43cd-a42f-f7082034304b",
   "metadata": {},
   "source": [
    "### Étape 5 : calculer la probabilité des phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980867c2-4e3c-4ae0-8bb0-44461cd21ed7",
   "metadata": {},
   "source": [
    "Ainsi, en appliquant le modèle défini à l’étape 3, on obtient les probabilités suivantes pour les phrases A, B et C :\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  P(A) &= 0,8 \\times 0,375 \\times 0,875 \\times 0,111 \\times 0,444 \\times 0,200 \\times 0,600 \\times 0,125 \\times 0,875 \\times 0,111 \\times 0,444 \\times 1\\\\\n",
    "  &\\approx 8,37 \\times 10^{-6}\\\\\n",
    "  P(B) &= 0,8 \\times 0,125 \\times 0,875 \\times 0,111 \\times 0,444 \\times 0,200 \\times 0,600 \\times 0,375 \\times 0,875 \\times 0,111 \\times 0,444 \\times 1\\\\\n",
    "  &\\approx 8,37 \\times 10^{-6}\\\\\n",
    "  P(C) &= 0,8 \\times 0,125 \\times 0,875 \\times 0,111 \\times 0,444 \\times 0,200 \\times 0\\\\\n",
    "  &= 0\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b0d1b0-9dde-44f9-89a3-4a25a70ee143",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46173f3-10e6-4513-ad24-d5b9165e5ff3",
   "metadata": {},
   "source": [
    "On observe que les phrases A et B ont exactement la même probabilité, bien que B soit hautement improbable d’un point de vue sémantique (une souris qui mange un chat). Cela montre que le modèle HMM ne prend en compte que les enchaînements de catégories grammaticales et non le sens des phrases.  \n",
    "\n",
    "À l’inverse, la phrase C, qui est parfaitement naturelle en français, est jugée impossible par le modèle simplement parce que la transition $P(\\text{PONCT} | \\text{VER})$ est absente du corpus d’apprentissage. Cela illustre une limitation des HMM : si une transition n’a jamais été observée, sa probabilité est nulle, ce qui exclut certaines phrases pourtant valides.  \n",
    "\n",
    "Un facteur clé dans ces observations est la **dépendance totale du modèle au corpus d’apprentissage**. Un HMM ne peut estimer des probabilités que sur ce qu’il a vu, ce qui pose problème si le corpus est trop restreint ou biaisé. Un corpus déséquilibré peut ainsi fausser les probabilités et entraîner des erreurs dans l’analyse des séquences. Pour améliorer la robustesse du modèle, il est essentiel d’avoir un corpus représentatif et d’utiliser des techniques de lissage. Le lissage de Laplace est une approche simple, mais souvent trop naïve car il attribue la même correction à toutes les probabilités. Des méthodes plus avancées, comme le **lissage additif optimisé**, le **lissage de Good-Turing** ou encore le **lissage de Kneser-Ney**, permettent une estimation plus fine des probabilités en tenant compte de la structure du corpus et des distributions observées.\n",
    "\n",
    "Les HMM d’ordre 1, comme celui utilisé ici, ne prennent en compte que la catégorie précédente pour prédire l’actuelle. Cette hypothèse simplificatrice empêche le modèle de capturer des **dépendances à long terme**, ce qui peut limiter ses performances dans certaines tâches linguistiques complexes. Des variantes, comme les **HMM d’ordre supérieur**, permettent d’intégrer plus de contexte mais augmentent aussi le coût computationnel.  \n",
    "\n",
    "Aujourd’hui, les modèles probabilistes comme les HMM ont été en grande partie remplacés par des approches plus avancées en **traitement automatique du langage naturel (TALN)**. Des modèles comme les **CRF (Conditional Random Fields)** offrent une alternative plus flexible en modélisant directement la distribution des séquences complètes, tandis que les **réseaux neuronaux récurrents (LSTM, GRU)** et les **modèles Transformers (BERT, GPT)** exploitent de vastes quantités de données et capturent des relations complexes entre mots, améliorant ainsi considérablement la modélisation du langage.  \n",
    "\n",
    "Enfin, les **langues morphologiquement riches**, comme le finnois ou l’arabe, posent un défi particulier aux HMM classiques, qui ne tiennent pas compte de la structure interne des mots. L’intégration d’informations morphologiques et syntaxiques devient alors essentielle pour obtenir une analyse plus fine des séquences.  \n",
    "\n",
    "Bien que les HMM aient des limites évidentes, ils restent une **base théorique précieuse** pour comprendre la modélisation des séquences et ont servi de fondation à de nombreuses avancées en TAL."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
