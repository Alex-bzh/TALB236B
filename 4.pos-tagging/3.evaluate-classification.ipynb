{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "617b6f18-ac08-42b3-a1a7-978a4d1393ef",
   "metadata": {},
   "source": [
    "# Comment juger de la qualité d’une tâche de classification ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509c9fe1-54bb-4a7a-b110-3431863221ae",
   "metadata": {},
   "source": [
    "Lorsque *TreeTagger* attribue une étiquette morpho-syntaxique à un mot-forme, il effectue une tâche de classification qui repose sur un modèle probabiliste. Mais comment savoir si le résultat est pertinent ou s’il est complètement à côté de la plaque ?\n",
    "\n",
    "Les métriques retenues vont dépendre de la tâche réalisée et, dans cet exercice, nous reconnaissons trois grandes familles :\n",
    "\n",
    "- La **classification binaire**, qui répond à la question : « L’observation appartient-elle à la classe cible ou non ? »\n",
    "- La **classification multi-classes**, qui répond à la question : « Parmi toutes, à quelle classe l’observation appartient-elle ? »\n",
    "- La **classification multi-étiquettes**, qui répond à la question : « L’observation appartient-elle à plusieurs classes ? »\n",
    "\n",
    "Dans le cas de *TreeTagger*, nous sommes en présence d’une classification multi-classes qui, à réfléchir de plus près, se révèle être un ensemble de classificateurs binaires pour lesquels nous pouvons mobiliser des métriques bien connues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40593cda-6a94-43dc-86bb-7c4ee4bf7dcb",
   "metadata": {},
   "source": [
    "## Quoi et comment évaluer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4342795a-cc2b-4767-a99f-36d37d6ad3d9",
   "metadata": {},
   "source": [
    "L’évaluation d’une tâche de classification ne s’effectue pas directement sur les prédictions d’un algorithme. Elle nécessite de les comparer avec des résultats connus. Par exemple, nous savons qu’un linguiste a évalué tel mot comme un adjectif et nous cherchons à savoir si notre modèle de langage a permis d’effectuer le même classement.\n",
    "\n",
    "Pour cela, il convient de disposer avant tout d’un jeu de données état de l’art, un *gold standard*, de préférence validé manuellement par des spécialistes du domaine. En linguistique, il en existe pour toutes les langues. Citons rapidement le *French Treebank* pour le français, le *Penn Treebank* ou le *Brown Corpus* pour l’anglais, le *Eindhoven-corpus* pour le néerlandais, le *Sinica Treebank* pour le mandarin, le *Qatar Arabic Corpus* pour l’arabe moderne etc.\n",
    "\n",
    "Lorsque l’on souhaite entraîner un algorithme sur un corpus de référence, il est coutume de séparer les données en deux parties : un jeu pour l’entraînement constitué de 80 % et un autre pour le test qui contiendra les 20 % restants. À ce stade, il est important de garder en mémoire que 100 % des données sont validées mais que le modèle entraîné n’en connaît qu’une partie.\n",
    "\n",
    "Une fois l’entraînement achevé, on utilise le modèle sur le jeu de test, en lui cachant les étiquettes réelles, avant de comparer les résultats. On ne se contentera jamais de vérifier la correspondance entre les étiquettes, mais, en mobilisant d’autres mesures, on essaiera de déterminer si les classements sont le fait du hasard ou d’un biais quelconque.\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "  A[(Corpus)]:::formal --> B{{Séparation des données}}:::formal\n",
    "  B -->|80%| C[/Jeu d'entraînement\\]:::academic\n",
    "  B -->|20%| D[/Jeu de test\\]:::academic\n",
    "classDef formal fill:#fff,stroke:#D68738,color:#D68738\n",
    "classDef academic fill:#032B4F,stroke:#032B4F,color:#fff\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f450c7-90a4-4d40-9b1d-749a756167b9",
   "metadata": {},
   "source": [
    "## Mesures générales de la performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b26c6b-07ab-4308-a5b0-cb9487da4231",
   "metadata": {},
   "source": [
    "### L’exactitude (*accuracy*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc66c70-1f4a-4616-9da5-6039f1388588",
   "metadata": {},
   "source": [
    "La toute première leçon est de ne jamais oublier qu’un très mauvais classificateur peut obtenir des résultats stupéfiants. Prenons l’exemple d’un jeu de données factice qui comporte une centaine d’images étiquetées manuellement selon deux modalités : *chat* ou *pas chat* :\n",
    "\n",
    "```csv\n",
    "image_001.jpg\tchat\n",
    "image_002.jpg\tpas chat\n",
    "image_003.jpg\tpas chat\n",
    "…\n",
    "image_100.jpg\tchat\n",
    "```\n",
    "\n",
    "Pour générer un tel fichier, nous pouvons recourir à l’utilitaire `awk`, puis éditer au hasard cinq lignes pour leur attribuer l’étiquette *chat* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcd771f-8380-428b-8431-fb508d240090",
   "metadata": {},
   "outputs": [],
   "source": [
    "! seq -f \"image_%03g.jpg\" 1 100 \\\n",
    "  | awk '{print $0 \"\\tpas chat\"}' > ./data/catalog.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a7d9ad-deed-4565-b7ad-4d169cc04fe1",
   "metadata": {},
   "source": [
    "Un comptage rapide, avec par exemple la commande `grep`, nous apprend que seulement cinq de ces images représentent des chats :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d327cb8a-ecc7-424a-89ed-07ef6e171896",
   "metadata": {},
   "outputs": [],
   "source": [
    "! grep -c '\\tchat' ./data/catalog.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc193144-3af9-4fc4-a1e7-b6a6166f9bce",
   "metadata": {},
   "source": [
    "Construisons maintenant un classificateur très naïf qui attribue systématiquement à toutes les images la classe *pas chat* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cc5c67-cc6a-421b-a5fe-03c562a10baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "! for i in $(seq -w 1 100); do \\\n",
    "    echo -e \"image_${i}.jpg\\tpas chat\"; \\\n",
    "done > ./data/output.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e821d1a-8288-4580-8065-0284d239a095",
   "metadata": {},
   "source": [
    "Si nous vérifions la correspondance entre les prédictions de notre algorithme et la réalité du jeu de données, nous obtenons un taux d’exactitude très satisfaisant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8721b7a6-cd51-4bba-bc59-4201f6de49f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! paste ./data/catalog.tsv ./data/output.tsv \\\n",
    "  | awk -F'\\t' '$2 == $4 {count++} END {print count}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609368fc-a844-4d8b-a45e-582579b1a978",
   "metadata": {},
   "source": [
    "**95 % !** Un taux à faire pâlir les diseuses de bonne aventure, non ? Pour cette raison, même s’il s’agit d’une première mesure incontournable, on ne se satisfera jamais du score d’exactitude, quitte même à s’en méfier dès que les jeux de données sont asymétriques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abb35ce-9871-4716-8982-92e41ac4b57e",
   "metadata": {},
   "source": [
    "### Une matrice de confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07812a2f-495b-495b-acfd-8b8c2b4a4e36",
   "metadata": {},
   "source": [
    "La matrice de confusion repose sur un principe simple : compter le nombre de fois où les observations ont été bien ou mal étiquetées. Elle révèle ainsi quatre informations essentielles :\n",
    "\n",
    "- les vrais positifs (*true positive*), le classificateur a repéré qu’il s’agissait d’un chat ;\n",
    "- les vrais négatifs (*true negative*), le classificateur a repéré qu’il ne s’agissait pas d’un chat ;\n",
    "- les faux positifs (*false positive*), le classificateur a cru qu’il s’agissait d’un chat ;\n",
    "- les faux négatifs (*false negative*), le classificateur aurait dû voir qu’il s’agissait d’un chat.\n",
    "\n",
    "Appliquons ces principes à notre exemple plus haut. Chaque ligne correspond à une classe réelle et chaque colonne à une classe prédite avec, sur la première ligne, la classe positive et, sur la seconde, la classe négative :\n",
    "\n",
    "| Classe réelle / prédite | Chat   | Pas Chat |\n",
    "|-------------------------|--------|----------|\n",
    "| **Chat**                | TP (0) | FN (5)   |\n",
    "| **Pas Chat**            | FP (0) | TN (95)  |\n",
    "\n",
    "**Explication des résultats :**\n",
    "\n",
    "- **TP (0) :** L’algorithme n’a jamais correctement identifié une image de chat.\n",
    "- **FN (5) :** L’algorithme a raté le classement des cinq images de chats en leur attribuant l’étiquette \"pas chat\".\n",
    "- **FP (0) :** L’algorithme n’a jamais prédit à tort qu’une image était celle d’un chat.\n",
    "- **TN (95) :** L’algorithme a correctement prédit 95 fois qu’il n’y avait pas de chat sur l’image.\n",
    "\n",
    "La matrice de confusion apporte des raffinements au score d’exactitude déterminé plus haut. Si de prime abord on pouvait se réjouir d’un taux de réussite de 95 %, on se rend compte ici des manques de notre classificateur naïf.\n",
    "\n",
    "Évaluer à partir de données brutes n’étant jamais évident, la matrice de confusion est le point de départ pour déterminer plusieurs métriques : la précision, la sensibilité (ou rappel) et le score $F_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45162bff-1e30-44fb-80a8-db2be13a5d11",
   "metadata": {},
   "source": [
    "## Évaluer un classificateur binaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ec917e-8f94-4b06-aad4-37d52c7d44c2",
   "metadata": {},
   "source": [
    "Dans le cas d’un projet basé sur un classificateur binaire, on fera toujours appel à une matrice de confusion à partir de laquelle on obtiendra d’autres métriques plus parlantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fff3c65-882e-44df-9d8e-674563a288cf",
   "metadata": {},
   "source": [
    "### La précision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5f904-9b26-4898-8418-9e987a40a088",
   "metadata": {},
   "source": [
    "La précision (*precision*) s’intéresse à l’exactitude des prédictions positives. Elle se calcule en effectuant le rapport entre les vrais positifs – les fois où le classificateur a correctement étiqueté la classe positive – et la somme de toutes les prédictions positives, incluant donc les fois où le classificateur s’est trompé. La formule vaut :\n",
    "\n",
    "$$\n",
    "\\text{precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "Le score de précision s’exprime sur un intervalle $[0, 1]$ où 1 signifie que toutes les prédictions positives faites par le modèle sont correctes (il n’y a aucun faux positif) et 0 que toutes les prédictions positives faites par le modèle sont incorrectes (aucun vrai positif).\n",
    "\n",
    "En résumé, un score élevé signifie que le modèle commet très peu d’erreurs en prédisant la classe positive alors qu’un score faible révèle qu’il prédit souvent à tort la classe positive.\n",
    "\n",
    "Pour notre exemple :\n",
    "\n",
    "$$\n",
    "\\text{precision} = \\frac{0}{0 + 0}\n",
    "$$\n",
    "\n",
    "Dans cette situation, nous obtenons une précision indéfinie (division par 0) que nous pouvons à juste titre interpréter comme une précision nulle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883e3e9b-8976-4183-b1e5-fc555a6961b2",
   "metadata": {},
   "source": [
    "### Le rappel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40c6f22-14f4-4f51-8941-94b500605c3c",
   "metadata": {},
   "source": [
    "Précision et rappel sont deux métriques complémentaires essentielles pour évaluer un classificateur binaire. Si la précision nous renseigne sur la fiabilité des prédictions positives, le rappel (*recall*), ou sensibilité, détermine la capacité du modèle à identifier correctement toutes les instances de la classe positive. Il s’obtient avec la formule :\n",
    "\n",
    "$$\n",
    "\\text{recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "Le rappel s’exprime également sur un intervalle $[0, 1]$ pour indiquer la capacité du modèle à trouver toutes les instances positives dans les données. Un modèle dont le rappel est proche de 1 détecte presque toutes les instances positives alors qu'un modèle avec un rappel faible laisse passer beaucoup d'instances positives sans les détecter.\n",
    "\n",
    "Pour notre exemple :\n",
    "\n",
    "$$\n",
    "\\text{recall} = \\frac{0}{0 + 5} = 0\n",
    "$$\n",
    "\n",
    "Notre classificateur n’a réussi à identifier aucune instance de la classe positive, ratant ainsi toutes les occurrences qui auraient dû être détectées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81469f35-383b-4618-b594-e3d7219f3c59",
   "metadata": {},
   "source": [
    "### Le score $F_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8f78e3-5fdc-440f-b84a-c1cbe1d93434",
   "metadata": {},
   "source": [
    "Le score $F_1$ effectue la moyenne harmonique entre la précision et le rappel selon la formule :\n",
    "\n",
    "$$\n",
    "F_1 = 2 \\cdot \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}}\n",
    "$$\n",
    "\n",
    "La moyenne harmonique donnant plus d’importance aux valeurs faibles, un classificateur ne pourra être bien noté par cette métrique que si sa précision et son rappel sont élevés. Cette métrique accorde un poids égal aux deux mesures, ce qui en fait un bon compromis lorsqu’aucune n’est à privilégier. Toujours établi dans un intervalle $[0,1]$, le score $F_1$ peut soit refléter un bon équilibre entre précision et rappel dans le cas où il se rapproche de 1, soit indiquer des problèmes à bien les équilibrer s’il est plutôt proche de 0. Signalons l’existence de variantes comme le score $F_\\beta$ qui permet de donner plus de poids au rappel ($\\beta > 1$) ou à la précision ($\\beta < 1$) selon les besoins spécifiques du problème.\n",
    "\n",
    "Pour notre exemple, nous obtenons :\n",
    "\n",
    "$$\n",
    "F_1 = 2 \\cdot \\frac{0 \\times 0}{0 + 0}\n",
    "$$\n",
    "\n",
    "Encore une fois, nous pouvons interpréter le $F_1$ indéfini comme étant nul."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f1ccbc-1fed-45df-bb47-74c9cdea3703",
   "metadata": {},
   "source": [
    "### Autres mesures de la performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e69ecd-1fb8-41d6-b2f7-0de0dc01b315",
   "metadata": {},
   "source": [
    "Il existe d’autres métriques d’évaluation que nous n’aborderons pas en détail ici. Certaines, comme la *Log Loss*, sont particulièrement adaptées aux modèles qui produisent des probabilités plutôt que des prédictions binaires directes. D’autres, comme les courbes ROC ou Précision-Rappel, permettent de visualiser les performances du modèle pour différents seuils de décision.\n",
    "\n",
    "Ces métriques incluent notamment :\n",
    "\n",
    "- La *Log Loss* (ou *Cross-entropy*) qui évalue la qualité des probabilités prédites.\n",
    "- La courbe ROC (*Receiver Operating Characteristic*) et son aire sous la courbe (AUC pour *Area Under the Curve*) qui visualisent le compromis entre vrais et faux positifs.\n",
    "- La courbe Précision-Rappel qui est particulièrement pertinente pour les jeux de données déséquilibrés."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a8fee1-80c1-4dc0-8589-a3cf63484e08",
   "metadata": {},
   "source": [
    "## Application à un cas réel : système de détection de fraude bancaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a0af75-06dc-4635-a862-b43bb7f26d1e",
   "metadata": {},
   "source": [
    "Pour illustrer l’importance de ces métriques, prenons l’exemple d’une banque qui souhaite détecter automatiquement les transactions frauduleuses. Dans ce contexte :\n",
    "\n",
    "- La classe positive représente les transactions frauduleuses ;\n",
    "- la classe négative représente les transactions légitimes.\n",
    "\n",
    "Analysons les différentes métriques pour comprendre leur pertinence business."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de365cb-1e9b-4762-a8bb-794c6644ce4c",
   "metadata": {},
   "source": [
    "### La précision dans ce contexte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc9fd65-1990-419e-b600-617a41994009",
   "metadata": {},
   "source": [
    "Une précision de 0,8 signifie que 80 % des transactions signalées comme frauduleuses par le système le sont réellement. Les 20 % restants sont des fausses alertes qui correspondent à des transactions légitimes incorrectement classées comme frauduleuses.\n",
    "\n",
    "Une faible précision implique que les équipes anti-fraude perdent du temps à investiguer des transactions légitimes et que les clients subissent des blocages injustifiés de leurs transactions. En revanche, une précision très élevée peut signifier que le système est trop conservateur : il applique un blocage systématique en présence d’une transaction incertaine, quitte à mécontenter les client·es à l’origine d’opérations parfaitement en règle. Cela pourrait également être le signe d’un surentraînement : le système ne repère que des cas évidents alors qu’il manque les cas plus subtils de fraude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e5f45a-24b0-4d53-a3a4-eae92e971513",
   "metadata": {},
   "source": [
    "### Le rappel dans ce contexte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d020763-27c9-4427-8f1a-3f0243f58fcb",
   "metadata": {},
   "source": [
    "Un rappel de 0,9 indique que le système détecte 90 % des fraudes réelles. Cependant, 10 % des transactions frauduleuses passent inaperçues.\n",
    "\n",
    "Un faible rappel est particulièrement problématique car chaque fraude non détectée représente une perte financière directe pour la banque ou ses clients. À l’inverse, un rappel très élevé s’obtient souvent au prix d’une précision dégradée."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0342c67-760b-40df-9d78-7dc56facddd0",
   "metadata": {},
   "source": [
    "### Le compromis précision-rappel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cd6e88-d9f9-4adc-924a-ade77438d4b9",
   "metadata": {},
   "source": [
    "Dans ce cas d’usage, le coût d’une fraude non détectée (faux négatif) est généralement plus élevé que celui d’une fausse alerte (faux positif) :\n",
    "\n",
    "- Un faux positif entraîne une investigation inutile et potentiellement un mécontentement client ;\n",
    "- un faux négatif conduit à une perte financière directe qui peut être conséquente.\n",
    "\n",
    "Pour cette raison, la banque pourrait choisir de privilégier le rappel par rapport à la précision, acceptant ainsi plus de fausses alertes pour minimiser le risque de rater des fraudes réelles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ec4325-77c0-4456-887a-0be1efac33bf",
   "metadata": {},
   "source": [
    "### Le score $F_1$ comme métrique de synthèse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a53ec5-5a4c-4513-afc4-2a038246801c",
   "metadata": {},
   "source": [
    "Le score $F_1$ permet d’évaluer rapidement si le système maintient un bon équilibre. Dans notre exemple, un score $F_1$ de 0,85 indiquerait que le système parvient à maintenir à la fois une bonne précision et un bon rappel, répondant ainsi efficacement aux objectifs business :\n",
    "\n",
    "- Détecter un maximum de fraudes ;\n",
    "- minimiser la charge de travail liée aux fausses alertes ;\n",
    "- limiter les blocages injustifiés de transactions légitimes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
